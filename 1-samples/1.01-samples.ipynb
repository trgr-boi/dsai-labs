{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e39257b0",
   "metadata": {},
   "source": [
    "# Module 1. Samples\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "- You are able to use Jupyter Notebooks (locally or in Google Colab) and configure your environment to run the code examples in this course.\n",
    "- You understand the basic terminology used in data science (indicated with ðŸ’¡).\n",
    "- You are able to load a dataset in Python and explore its structure and characteristics.\n",
    "\n",
    "## Getting Started with Jupyter Notebooks\n",
    "\n",
    "Jupyter Notebooks (or iPython Notebooks) are files with a mixture of Markdown and Python code. It's a very convenient format so we'll be using this quite a lot during this course!\n",
    "\n",
    "**Read the instructions below carefully, as they do not necessarily apply to all environments!**\n",
    "\n",
    "### Google Colab\n",
    "\n",
    "To get started, it is not necessary to install software on your laptop. [Google Colab](https://colab.research.google.com/) is a free online iPython development environment, but it does require a Google account in order to use it. You can create a local clone of the Github repo with the lab assignments and upload the Notebook files you want to work with to Colab.\n",
    "\n",
    "Note that the Google Colab environment has older versions installed of the Python libraries that we will be using throughout this course. Usually, that's not a problem, but sometimes you will run into errors, e.g. because some parameter in our Python code didn't exist yet in that version. The solution is to add a new code block at the beginning of your Notebook file with content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9dfa18",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade matplotlib\n",
    "%pip install --upgrade scipy\n",
    "%pip install --upgrade statsmodels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "45eea59b",
   "metadata": {},
   "source": [
    "You only have to do this in Notebooks where you encounter errors, and only for the libraries that cause these errors. The installation may take a while and you need to restart the Python runtime afterwards by clicking the button that appears.\n",
    "\n",
    "### Local environment\n",
    "\n",
    "If you prefer to have a local development environment, you need to install Python and Visual Studio Code with the necessary extensions for working with Python (e.g. Pylance, Jupyter, Jupyter Keymap, etc.) The first time you open a Notebook file, VSCode will offer you to install these extensions.\n",
    "\n",
    "### Windows\n",
    "To install Python and VSCode on Windows, we recommend using `winget`. First, check the latest version of Python on the [Python website](https://www.python.org/downloads/windows/). For example, at the time of writing, the latest version was 3.14.2. We'll need to specify the major and minor version (e.g. 3.14) when installing. Then, open a PowerShell or command prompt as Administrator and run:\n",
    "\n",
    "```console\n",
    "> winget install Microsoft.VisualStudioCode\n",
    "> winget install Python.Python.3.14\n",
    "```\n",
    "\n",
    "Remark that `winget upgrade --all` will install maintenance releases of Python, but it will not upgrade to a new major or minor version."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1074877",
   "metadata": {},
   "source": [
    "### Linux\n",
    "\n",
    "Some Linux distributions no longer allow you to use `pip` to install Python packages globally. In that case, we recommend to work with a Python Virtual Environment. VS Code will help you to create one when you select the Python version to use for executing code cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d6bea2",
   "metadata": {},
   "source": [
    "### MacOS\n",
    "\n",
    "The install Python on MacOS, it's best to use [Homebrew](https://brew.sh/). After installing issue the following commands.\n",
    "\n",
    "```console\n",
    "> brew install python\n",
    "> export PATH=\"/opt/homebrew/opt/python/libexec/bin:$PATH\"\n",
    "```\n",
    "You probably want to add the `export PATH` line to your `.zshrc` (or whichever shell you use)\n",
    "\n",
    "You can check everything is correctly set through\n",
    "\n",
    "```console\n",
    "> which python\n",
    "/opt/homebrew/opt/python/libexec/bin/python\n",
    "```\n",
    "\n",
    "It's always a good idea to set up a venv\n",
    "\n",
    "```console\n",
    "dsai-labs> python3 -m venv .venv\n",
    "source .venv/bin/activate\n",
    "```\n",
    "\n",
    "You should now be able to use this environment as your kernel environment in VSCode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0f50da",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "\n",
    "Most Python scripts (or Jupyter Notebooks) for analyzing data use the same program libraries. The most important ones are:\n",
    "\n",
    "- `numpy` - multidimensional arrays, linear algebra, etc.\n",
    "- `scipy` - mathematics, science, engineering etc.\n",
    "- `pandas` - data-analysis and -manipulation\n",
    "- `matplotlib`, `seaborn` - data visualisation\n",
    "- `scikit-learn` - regression and machine learning\n",
    "\n",
    "You can install these using `pip`:\n",
    "\n",
    "```console\n",
    "> pip install matplotlib numpy pandas scipy seaborn scikit-learn statsmodels\n",
    "```\n",
    "\n",
    "Or, alteratively, by executing the following code block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09baba66",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install matplotlib\n",
    "%pip install numpy\n",
    "%pip install pandas\n",
    "%pip install scipy\n",
    "%pip install seaborn\n",
    "%pip install scikit-learn\n",
    "%pip install statsmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bf2e22",
   "metadata": {},
   "source": [
    "\n",
    "Since you usually need the same packages every time, it is best to put them at the top of every script or Notebook file you write. By convention, package names are often abbreviated in a consistent manner, e.g. `np` for `numpy`, `sns` for `seaborn`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5195c19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary packages\n",
    "import numpy as np                                  # \"Scientific computing\"\n",
    "import scipy.stats as stats                         # Statistical tests\n",
    "\n",
    "import pandas as pd                                 # Data Frame\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "import matplotlib.pyplot as plt                     # Basic visualisation\n",
    "from statsmodels.graphics.mosaicplot import mosaic  # Mosaic diagram\n",
    "import seaborn as sns                               # Advanced data visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b297eec",
   "metadata": {},
   "source": [
    "**Tip:** you can define a code snippet in VSCode for this block of code, so you don't have to type it every time. See e.g. [this guide](https://code.visualstudio.com/docs/editor/userdefinedsnippets) on how to create code snippets in VSCode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0cd509",
   "metadata": {},
   "source": [
    "## Basic Concepts in Data Science\n",
    "\n",
    "### Variables and Measurement Levels\n",
    "\n",
    "In statistics and data science, a ðŸ’¡**variable** is any attribute of an object of study. Variables can have ðŸ’¡**values** that differ between objects. Usually, variables are classified into different types, or ðŸ’¡**measurement levels**, depending on the kind of values they can take and the mathematical operations that can be performed on them. The most common types are:\n",
    "\n",
    "- ðŸ’¡**Qualitative** (or categorical) variables: variables that can take on a limited number of distinct categories or labels. These are not necessarily numerical.\n",
    "\n",
    "    - ðŸ’¡**Nominal** variables: categorical variables *without a specific order* or ranking among the categories (e.g., colors, types of animals).\n",
    "    - ðŸ’¡**Ordinal** variables: categorical variables *with a specific order* or ranking among the categories (e.g., education level, customer satisfaction ratings).\n",
    "\n",
    "- ðŸ’¡**Quantitative** (or numerical) variables: variables that can take on numerical values, which can be discrete (countable values) or continuous (any value within a range). These are usually the result of measurements. Often, there are many different values of quantitative variables in a dataset, or they may even be unique.\n",
    "\n",
    "    - ðŸ’¡**Interval** variables: numerical variables where the intervals between values are meaningful, but there is no true zero point (e.g., temperature in Celsius or Fahrenheit).\n",
    "    - ðŸ’¡**Ratio** variables: numerical variables with a true zero point, allowing for meaningful ratios between values (e.g., height, weight, age).\n",
    "\n",
    "It is important to correctly identify the type of variable you are working with, as this influences the choice of statistical methods and visualizations used for analysis. This is something that a computer is not very good at, so you often need to make these distinctions yourself when exploring and analyzing a dataset.\n",
    "\n",
    "### Populations, samples, observations\n",
    "\n",
    "In data science and statistics, a ðŸ’¡**population** refers to the entire set of individuals or items that you are interested in studying. A ðŸ’¡**sample** is a subset of the population that is selected for analysis. A single individual or item within a population or sample is called an ðŸ’¡**observation**.\n",
    "\n",
    "The goal of sampling is to gather information about the population without having to study every individual or item within it. Under certain conditions (which will be explored later in this course), you can use the information obtained from a sample to make inferences about the entire population. This is often necessary because studying the entire population can be impractical, time-consuming, or expensive.\n",
    "\n",
    "When selecting a sample from a population, it is important to ensure that the sample has specific characteristics. Specifically, a sample should be:\n",
    "\n",
    "- ðŸ’¡**Randomly selected**, i.e. every individual or item in the population should have an *equal chance of being included* in the sample in order to reduce bias, but randomness alone does not guarantee that a sample is perfectly representative - especially for small sample sizes..\n",
    "- **Large enough** to provide reliable estimates of population parameters. We will explore this more in chapter 3.\n",
    "- ðŸ’¡**Representative** of the population, meaning that it should accurately reflect the characteristics of the population as a whole.\n",
    "\n",
    "Ideally, a researcher would do the following to draw a sample from a population:\n",
    "\n",
    "- First and foremost, the population must be well defined\n",
    "- Then you can compile a ðŸ’¡**sampling frame**, i.e. a list of all individuals in the population\n",
    "- Then, the researcher randomly selects a number of individuals from this list.\n",
    "\n",
    "It is often difficult or impossible to set up a sampling frame. In such cases, the researcher chooses a different sampling method and tries to approximate the properties listed above as closely as possible within the available time and budget.\n",
    "\n",
    "### Sampling errors\n",
    "\n",
    "\n",
    "Even under the best of circumstances, the result within a sample is likely to deviate from the \"true\" value in the population, although we hope we can approach it sufficiently. So by taking a sample we inevitably introduce errors. A useful way to categorize errors is given below:\n",
    "\n",
    "- ðŸ’¡**Sampling errors** are caused by the way we take a sample. We distinguish between ðŸ’¡*random* or ðŸ’¡*systematic* sampling errors:\n",
    "\n",
    "    - **Random sampling errors** are only due to chance and cannot be avoided. Every time you take a random sample, you will get a different result within that sample.\n",
    "\n",
    "    - **Systematic sampling errors** are worse, because they can cause the results to be biased and therefore no longer be *representative* of a population. Examples:\n",
    "\n",
    "        - Online survey: people without internet are excluded\n",
    "\n",
    "        - Street survey: only people who are currently walking there\n",
    "\n",
    "        - Voluntary survey: only interested parties participate\n",
    "\n",
    "- ðŸ’¡**Non-sampling errors** are errors that occur after the sample has been taken, e.g. in the measurement and analysis. In this case, we speak of non-sampling errors and again we distinguish between *random* and *systematic*:\n",
    "\n",
    "    - **Random non-sampling errors** For example, a respondent accidentally selects or fills out a wrong answer in a survey.\n",
    "\n",
    "    - **Systematic non-sampling errors** For example:\n",
    "\n",
    "        - a measuring device that is not calibrated correctly and systematically gives a slightly higher value than the true value. \n",
    "\n",
    "        - respondents intentionally underestimate or overestimate reality (e.g. when asked about how much you smoke or drink alcohol on a daily basis)\n",
    "\n",
    "### Representative samples\n",
    "\n",
    "A sample is said to be ðŸ’¡**representative** of a population when the characteristics of the sample closely match those of the population. This means that the sample accurately reflects the diversity and distribution of key variables within the population, allowing for valid inferences to be made about the entire population based on the sample data.\n",
    "\n",
    "In a ðŸ’¡**stratified** sample, the population is divided into subgroups (or strata) based on certain characteristics, and samples are drawn from each stratum in proportion to their representation in the population. This helps ensure that the sample is representative of the population as a whole, especially when certain subgroups may be underrepresented in a simple random sample.\n",
    "\n",
    "For example, if a population of 5810 individuals consists of the following age groups and genders:\n",
    "\n",
    "|           | $\\le 18$ | $]18,25]$ | $]25, 40]$ | $> 40$ | Total |\n",
    "| --------: | -------: | --------: | ---------: | -----: | ----: |\n",
    "|     Woman |      500 |      1500 |       1000 |    250 |  3250 |\n",
    "|       Man |      400 |      1200 |        800 |    160 |  2560 |\n",
    "| **Total** |      900 |      2700 |       1800 |    410 |  5810 |\n",
    "\n",
    "A stratified sample of 10% would then consist of 581 individuals, with the following distribution:\n",
    "\n",
    "|           | $\\le 18$ | $]18,25]$ | $]25, 40]$ | $> 40$ | Total |\n",
    "| --------: | -------: | --------: | ---------: | -----: | ----: |\n",
    "|     Woman |       50 |       150 |        100 |     25 |   325 |\n",
    "|       Man |       40 |       120 |         80 |     16 |   256 |\n",
    "| **Total** |       90 |       270 |        180 |     41 |   581 |\n",
    "\n",
    "In chapter 4, we will discuss a method that allows us to verify whether a sample is representative of a population.\n",
    "\n",
    "### Univariate vs multivariate analysis\n",
    "\n",
    "In chapter 2, on ðŸ’¡**univariate analysis**, we will focus on analyzing and summarizing data for a single variable at a time. This involves techniques such as calculating measures of central tendency (*mean*, *median*, *mode*) and measures of dispersion (*range*, *variance*, *standard deviation*), as well as creating visualizations like histograms and box plots to understand the distribution of the variable.\n",
    "\n",
    "In chapters 4 to 6, we will explore various methods to investigate whether two variables have some kind of relationship or association with each other, under the umbrella term of ðŸ’¡**bivariate analysis**.\n",
    "\n",
    "The appopriate analysis or visualization technique always depends on the measurement levels of the variables involved. For example, we might use a scatter plot to visualize the relationship between two quantitative variables, or a contingency table to examine the association between two categorical variables.\n",
    "\n",
    "We say that two variables have an ðŸ’¡**association** or a ðŸ’¡**relationship** when the value of one variable provides some information about the value of the other variable. For example, there may be an association between age and income, where older individuals tend to have higher incomes. The variable that provides information about the other variable is called the ðŸ’¡**independent variable**, while the variable being predicted or explained is called the ðŸ’¡**dependent variable**.\n",
    "\n",
    "It is essential to understand that when you find an association between two variables, it does not necessarily imply a causal relationship. A phrase often used in this context is **correlation is not causation**. In other words, just because two variables are associated does not mean that one variable causes the other to change. There may be other factors at play, or the association could be entirely coincidental (*spurious correlation*). See some examples [here](https://plotlygraphs.medium.com/spurious-correlations-56752fcffb69) and [here](https://www.tylervigen.com/spurious-correlations)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcddffd8",
   "metadata": {},
   "source": [
    "## Opening a Dataset, General Information\n",
    "\n",
    "Throughout this course, we will load ðŸ’¡**datasets** from various sources to practice our data analysis skills. A dataset usually corresponds to a *sample* taken from a *population* being studied, and is typically organized in a tabular format where rows represent individual *observations* and columns represent different *variables*.\n",
    "\n",
    "To read a dataset into Python, you can use the `pandas` library, which provides powerful tools for data manipulation and analysis. Pandas has several functions to read data from various file formats, such as CSV (`read_csv()`), Excel (`read_excel()`), JSON (`read_json()`), and more.\n",
    "\n",
    "You can specify a path to a local file or even a URL to download the dataset directly from the web:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "192d66fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the Titanic dataset. (Rajagopalan, 2021, p. 106)\n",
    "titanic = pd.read_csv('https://raw.githubusercontent.com/DataRepo2019/Data-files/master/titanic.csv')\n",
    "# Show the first few records of the Data Frame\n",
    "titanic.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e377baca",
   "metadata": {},
   "source": [
    "If you want to open a dataset that you stored on Google Drive (where Google Colab stores all your Notebooks), you can use the following code instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556425a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure your Google Drive is accessible from within the notebook\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Load the dataset from your Google Drive (here, we stored it in a subfolder\n",
    "# called \"data\")\n",
    "titanic = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/data/titanic.csv')\n",
    "titanic.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1ef393a2",
   "metadata": {},
   "source": [
    "Let's take a look at the properties of the dataset we just loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e3aad78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 891\n",
      "Number of columns: 12\n",
      "The shape of the Data Frame is: (891, 12)\n",
      "**************************************************\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n",
      "**************************************************\n",
      "PassengerId      int64\n",
      "Survived         int64\n",
      "Pclass           int64\n",
      "Name            object\n",
      "Sex             object\n",
      "Age            float64\n",
      "SibSp            int64\n",
      "Parch            int64\n",
      "Ticket          object\n",
      "Fare           float64\n",
      "Cabin           object\n",
      "Embarked        object\n",
      "dtype: object\n",
      "**************************************************\n",
      "int64      5\n",
      "object     5\n",
      "float64    2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# How many  rows does the DataFrame have?\n",
    "print(f\"Number of rows: {len(titanic)}\")\n",
    "# How many columns?\n",
    "print(f\"Number of columns: {len(titanic.columns)}\")\n",
    "# How many rows and columns, i.e. the shape\n",
    "print(f\"The shape of the Data Frame is: {titanic.shape}\")\n",
    "# General information about the DataFrame\n",
    "print(\"*\"*50)\n",
    "titanic.info()\n",
    "\n",
    "# Give the data type of each column.\n",
    "print(\"*\"*50)\n",
    "print(titanic.dtypes)\n",
    "\n",
    "# How many columns of each data type are there?\n",
    "#   Watch it! The book says to use get_dtype_counts(), but this method no longer exists\n",
    "print(\"*\"*50)\n",
    "print(titanic.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391ec95c",
   "metadata": {},
   "source": [
    "### Indices\n",
    "\n",
    "The column \"PassengerId\" is not an actual variabele, but contains a number to identify each observation. You can mark this column as an index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78d18a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.set_index(['PassengerId'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d3425b",
   "metadata": {},
   "source": [
    "### Qualitative variables\n",
    "\n",
    "Earlier, we stated that the computer is not always able to correctly identify the measurement levels of variables. In this dataset, some of the variables, such as `Survived` and `Pclass`, are incorrectly considered to be quantitative. You can correct this by explicitly converting them to a **qualitative** (categorical) variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a192c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the variable Survived -> is treated as quantitative\n",
    "print(titanic.Survived.describe())\n",
    "# Convert to a categorical variable\n",
    "titanic.Survived = titanic.Survived.astype('category')\n",
    "# Call desribe() again -> now it is treated as qualitative\n",
    "print(titanic.Survived.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96881d1",
   "metadata": {},
   "source": [
    "You can also mark variables as **ordinal**, that is, with an ordering. We will do this as an example with the variable \"Embarked\" and order the ports by the order in which the Titanic stopped there. The Titanic departed at Southhampton, and then picked up passengers first at Cherbourg and then at Queenstown.\n",
    "\n",
    "For cases like this, define your own datatype specifying the order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2223ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(titanic.Embarked.unique())\n",
    "\n",
    "embarked_type = CategoricalDtype(categories=['S', 'C', 'Q'], ordered=True)\n",
    "titanic.Embarked = titanic.Embarked.astype(embarked_type)\n",
    "titanic.Embarked.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1808f375",
   "metadata": {},
   "source": [
    "This order will then always be respected, e.g. in graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4af2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data=titanic, x='Embarked');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ccfdf9",
   "metadata": {},
   "source": [
    "### Selecting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22945701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all observations for a single variable (i.e. a DataFrame column)\n",
    "titanic.Age\n",
    "# This also works (and is preferable as it will also work when the column name has a space in it):\n",
    "# titanic['Age']\n",
    "# This also works, but isn't very nice\n",
    "# titanic.loc[:, 'Age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943d33b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select adjacent columns\n",
    "titanic.iloc[:, 2:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5267f2d9",
   "metadata": {},
   "source": [
    "You can also select multiple columns based on their names.\n",
    "This is often clearer than selecting based on position and the columns must \n",
    "not be adjacent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3071015",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic[['Name', 'Age', 'Cabin']] # Note: two sets of square brackets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef46fd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observation with row number 5 (counting from zero)\n",
    "print(titanic.iloc[5])\n",
    "\n",
    "# The first 4 observations\n",
    "titanic.iloc[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f94a60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select observations where the value of Age is less than 18\n",
    "titanic[titanic.Age < 18]  \n",
    "\n",
    "# The same, but only keep the column 'Embarked'\n",
    "titanic[titanic.Age < 18].Embarked\n",
    "\n",
    "# The same, but keep columns 'Age' and 'Embarked'\n",
    "titanic[titanic['Age'] < 18][['Age', 'Embarked']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f74c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all boys younger than 10\n",
    "titanic.query(\"(Sex=='male') and (Age < 18)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729b425a",
   "metadata": {},
   "source": [
    "### Dropping Data and Working with Missing Data\n",
    "\n",
    "Pandas tries to make working with missing data as easy as possible. E.g., all of the descriptive statistics on pandas objects exclude missing data by default."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b3c6e1",
   "metadata": {},
   "source": [
    "Let's start by reading the titanic data again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb502f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = pd.read_csv('https://raw.githubusercontent.com/DataRepo2019/Data-files/master/titanic.csv')\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bc827a",
   "metadata": {},
   "source": [
    "The `PassengerId` column is identical to the index hence it doesn't provide any additional useful information. Let's drop this column (rather than setting it as the index as before). This can be done using `drop`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7db6a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.drop(\"PassengerId\", axis=\"columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2dd910",
   "metadata": {},
   "source": [
    "Notice that we didn't actually change the `titanic` `DataFrame`.  We can either assign the result of `drop` to a (new) `DataFrame`\n",
    "or we can use `inplace=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e010cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.head(3) # PassengerId is still here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824c9303",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = titanic.drop(\"PassengerId\", axis=\"columns\")\n",
    "titanic.head(3) # PassengerId is now gone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d91b4b",
   "metadata": {},
   "source": [
    "From the output of the `info` method we can infer that certain columns contain many missing values, e.g. `Cabin` only contains 204 non-missing values. \n",
    "For the purposes of illustration, let's drop any row that has a missing observation by using \n",
    "the default behaviour of the `dropna` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8bc8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e5a55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = titanic.dropna() # Drop any row that has at least one missing value\n",
    "print(cleaned.info())\n",
    "cleaned.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a344447",
   "metadata": {},
   "source": [
    "This would only keep 183 values, which isn't a lot. We could also choose to drop only the rows that consist of nothing but missing values (there are no such rows in the `titanic` `DataFrame`), but here you can see how this would be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6b8687",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = titanic.dropna(how=\"all\")\n",
    "cleaned.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b75af0",
   "metadata": {},
   "source": [
    "Instead of relying on `info` to compute the number of non-missing values we can also use the `isnull` and/or `notnull` methods.\n",
    "`notnull` computes a boolean `DataFrame` where an entry is `True` if and only if that entry is considered to be non-missing in the original `DataFrame`. We can then use `sum` to compute the sum of each column. This gives the number of non-missing values per column.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ee0c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_null_df = titanic.notnull()\n",
    "print(not_null_df.tail(3))\n",
    "print(\"Number of non null values in each column:\")\n",
    "print(not_null_df.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd4d0df",
   "metadata": {},
   "source": [
    "An even easier alternative is to simply use `count` on the `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd1d2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(titanic.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3606a9",
   "metadata": {},
   "source": [
    "The `Cabin` column has too many missing values, so probably isn't useful. For the `Age` column, we can *impute* the missing values, e.g. with the average age of all passengers. (We will see later what this means exactly). Let's do this using `fillna`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b356dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First compute the average age\n",
    "avg_age = titanic['Age'].mean()\n",
    "print(f\"(Rounded) Average age of passengers: {round(avg_age)}\")\n",
    "titanic = titanic.fillna(value={'Age' : avg_age})\n",
    "print(\"We should now confirm that the 'Age' column no longer has missing values\")\n",
    "titanic.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37e5ee4",
   "metadata": {},
   "source": [
    "### Creating New Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb17e2b",
   "metadata": {},
   "source": [
    "We use a dataset containing $NO_2$ measurements at the stations of Paris, Antwerp and London.\n",
    "\n",
    "Let's read the data and check whether it worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca0d872",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://raw.githubusercontent.com/pandas-dev/pandas/master/doc/data/air_quality_no2.csv\"\n",
    "air_quality = pd.read_csv(URL)\n",
    "print(air_quality.info())\n",
    "air_quality.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453be401",
   "metadata": {},
   "source": [
    "We see that \n",
    "- the dates are stored as strings (object dtype)\n",
    "- `datetime` is added as a column and we have a `RangeIndex`.\n",
    "\n",
    "We would like to\n",
    "- parse the dates, so that they are available as `datetime` objects\n",
    "- index the `DataFrame` rows by this date.\n",
    "\n",
    "This can be achieved by specifying the index column and by indicating that dates must be parsed on `read_csv`. Let's try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367332d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality = pd.read_csv(URL, index_col=0, parse_dates=True)\n",
    "print(air_quality.info())\n",
    "air_quality.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9e5401",
   "metadata": {},
   "source": [
    "We can clearly see the difference!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d74337",
   "metadata": {},
   "source": [
    "### Creating a New Column Derived from (an) Existing Column(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e51741e",
   "metadata": {},
   "source": [
    "Assume we would like to express the concentration of the station in London in mg/m^3\n",
    "\n",
    "*(If we assume temperature of 25 degrees Celsius and pressure of 1013 hPa, the conversion factor is 1.882)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08fc868",
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality[\"london_mg_per_cubic\"] = air_quality[\"station_london\"] * 1.882\n",
    "air_quality.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f844a838",
   "metadata": {},
   "source": [
    "To create a new column, use the `[]` brackets with the new column name at the left side of the assignment.  The computation is done **element wise**, hence there is no need for an explicit loop!\n",
    "\n",
    "We can also use multiple columns to derive a new column.\n",
    "\n",
    "Let's check the ratio of the values in Paris versus Antwerp and save the result in a new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a38417",
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality[\"ratio_paris_antwerp\"] = air_quality[\"station_paris\"] / air_quality[\"station_antwerp\"]\n",
    "air_quality.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b24c220",
   "metadata": {},
   "source": [
    "We can also perform more general mappings on columns, either using a function or a dictionary.\n",
    "\n",
    "Let's work with the Titanic data once more.  Let's say we want to numerically encode the `Sex` column, this can be done as follows. First let's check the unique values in the `Sex` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173b9272",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic['Sex'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab92e86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_dict = {'male' : 0, 'female' : 1}\n",
    "titanic['Sex'] = titanic['Sex'].map(mapping_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f427f9",
   "metadata": {},
   "source": [
    "Let's check whether it worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6401d15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5a51ac",
   "metadata": {},
   "source": [
    "Let's add a new feature (i.e. a new column). When `Age` is less than 12 we will call this passenger a `child`, \n",
    "between 12 and 18 a `teen` and over 18 will be `adult`s.  First, we define a Python function implementing \n",
    "this mapping, and then we apply it to the `Age` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcd3784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def age_to_category(age):\n",
    "    if age < 12:\n",
    "        return \"child\"\n",
    "    if age < 18:\n",
    "        return \"teen\"\n",
    "    return \"adult\"\n",
    "\n",
    "titanic['AgeCategory'] = titanic['Age'].map(age_to_category)\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f83ed19",
   "metadata": {},
   "source": [
    "Here, we only scratched the surface of what is possible with pandas (and indeed, whole books have been written about pandas). In general the pandas getting started guide, the user guide and the API reference are very good resources to find additional information. You can find \n",
    "all of these [here](https://pandas.pydata.org/pandas-docs/stable/index.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
